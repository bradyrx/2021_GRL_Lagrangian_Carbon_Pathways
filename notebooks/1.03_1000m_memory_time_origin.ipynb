{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.03 1000 m Memory Time Origin \n",
    "\n",
    "---\n",
    "\n",
    "Author : Riley X. Brady\n",
    "\n",
    "Date : 11/18/2020\n",
    "\n",
    "This computes the statistical origin of DIC for the particle's 1000 m crossing point, based on the $e$-folding memory time. It appends on the x, y, and z location at the origin as well as a host of tracers and the length of the memory time for the given particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import figutils\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.19.4\n",
      "xarray: 0.16.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"xarray: {xr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my TCP client from the `launch_cluster` notebook. I use it\n",
    "# for distributed computing with `dask` on NCAR's machine, Casper.\n",
    "client = Client(\"tcp://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all particles that were pre-filtered to those that cross 1000 m within the ACC.\n",
    "\n",
    "**Note**: I loaded in the netCDF file, and chunked it, and then saved it back out as a `zarr` file. This makes `dask` run a lot more efficiently. E.g.,\n",
    "\n",
    "```python\n",
    "ds = xr.open_dataset('../data/southern_ocean_deep_upwelling_particles.nc')\n",
    "ds = ds.chunk({'time': -1, 'nParticles': 'auto'})\n",
    "ds.to_zarr('../data/southern_ocean_deep_upwelling_particles.zarr', consolidated=True)\n",
    "```\n",
    "\n",
    "You could probably chunk the particles into slightly smaller chunks for even faster performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the `zarr` file, which is pre-chunked and already has been\n",
    "# filtered from the original 1,000,000 particles to the 19,002 that\n",
    "# upwell last across 1000 m S of 45S and outside of the annual sea ice\n",
    "# edge.\n",
    "filepath = \"../data/southern_ocean_deep_upwelling_particles.zarr/\"\n",
    "ds = xr.open_zarr(filepath, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in 1000 m crossing locations\n",
    "crossings = xr.open_dataset(\"../data/postproc/1000m.crossing.locations.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to compute the memory time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr_by_hand(x, lag):\n",
    "    \"\"\"Computes the autocorrelation coefficient.\n",
    "\n",
    "    See:\n",
    "    https://stackoverflow.com/questions/36038927/\n",
    "    whats-the-difference-between-pandas-acf-and-statsmodel-acf\n",
    "\n",
    "    x : numpy time series (here it'll be particleDIC)\n",
    "    lag : int of the lag for which to autocorrelate.\n",
    "    \"\"\"\n",
    "    # Slice the relevant subseries based on the lag\n",
    "    y1 = x[: (len(x) - lag)]\n",
    "    y2 = x[lag:]\n",
    "    # Subtract the subseries means\n",
    "    sum_product = np.sum((y1 - np.mean(y1)) * (y2 - np.mean(y2)))\n",
    "    # Normalize with the subseries stds\n",
    "    return sum_product / ((len(x) - lag) * np.std(y1) * np.std(y2))\n",
    "\n",
    "\n",
    "def compute_idx_of_last_1000m_crossing(z):\n",
    "    \"\"\"Find index of final time particle upwells across 1000 m.\n",
    "\n",
    "    z : zLevelParticle (m)\n",
    "    \"\"\"\n",
    "    currentDepth = z\n",
    "    previousDepth = np.roll(z, 1)\n",
    "    previousDepth[0] = 999  # So we're not dealing with a nan here.\n",
    "    cond = (currentDepth >= -1000) & (previousDepth < -1000)\n",
    "    idx = (\n",
    "        len(cond) - np.flip(cond).argmax() - 1\n",
    "    )  # Finds last location that condition is true.\n",
    "    return idx\n",
    "\n",
    "\n",
    "def deep_tracers_origin(x, y, z, dic, T, S, alk, po4, sio3):\n",
    "    \"\"\"Finds x, y, z statistical origin and content of tracer before its\n",
    "    last 1000 m crossing.\n",
    "\n",
    "    Memory time is decided by the e-folding time.\n",
    "\n",
    "        x (ndarray): lonParticle\n",
    "        y (ndarray): latParticle\n",
    "        z (ndarray): zLevelDepth\n",
    "        tracer (ndarray): e.g., particleDIC\n",
    "    \"\"\"\n",
    "    # Find last crossing to 1000m and trim to time series leading up\n",
    "    # to this point.\n",
    "    idx = compute_idx_of_last_1000m_crossing(z)\n",
    "    x_subset = x[0:idx]\n",
    "    y_subset = y[0:idx]\n",
    "    z_subset = z[0:idx]\n",
    "    dic_subset = dic[0:idx]\n",
    "    T_subset = T[0:idx]\n",
    "    S_subset = S[0:idx]\n",
    "    alk_subset = alk[0:idx]\n",
    "    po4_subset = po4[0:idx]\n",
    "    sio3_subset = sio3[0:idx]\n",
    "\n",
    "    # Compute autocorrelation function based on DIC for all lags.\n",
    "    auto = np.asarray([autocorr_by_hand(dic_subset, i) for i in range(len(dic_subset))])\n",
    "    # Find the time step at which the autocorrelation drops below 1/e.\n",
    "    e_folding = (auto <= (1 / np.e)).argmax()\n",
    "\n",
    "    # Go backward this many steps.\n",
    "    subset_idx = len(z_subset) - e_folding - 1\n",
    "\n",
    "    # If there is no zero crossing, argmax will return exactly zero. Which can't be the\n",
    "    # case ever since at zero, ACF == 1 by definition.\n",
    "    if e_folding == 0:\n",
    "        x_origin = np.nan\n",
    "        y_origin = np.nan\n",
    "        z_origin = np.nan\n",
    "        dic_origin = np.nan\n",
    "        T_origin = np.nan\n",
    "        S_origin = np.nan\n",
    "        alk_origin = np.nan\n",
    "        po4_origin = np.nan\n",
    "        sio3_origin = np.nan\n",
    "        memory_time = np.nan\n",
    "    else:\n",
    "        x_origin = x_subset[subset_idx]\n",
    "        y_origin = y_subset[subset_idx]\n",
    "        z_origin = z_subset[subset_idx]\n",
    "        dic_origin = dic_subset[subset_idx]\n",
    "        T_origin = T_subset[subset_idx]\n",
    "        S_origin = S_subset[subset_idx]\n",
    "        alk_origin = alk_subset[subset_idx]\n",
    "        po4_origin = po4_subset[subset_idx]\n",
    "        sio3_origin = sio3_subset[subset_idx]\n",
    "        memory_time = e_folding * 2  # approximate memory time in days.\n",
    "    return np.array(\n",
    "        [\n",
    "            x_origin,\n",
    "            y_origin,\n",
    "            z_origin,\n",
    "            dic_origin,\n",
    "            T_origin,\n",
    "            S_origin,\n",
    "            alk_origin,\n",
    "            po4_origin,\n",
    "            sio3_origin,\n",
    "            memory_time,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive memory time origin for each region, and then the remaining particles that don't upwell in a topographic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_locations_and_save_dataset(ensemble, region):\n",
    "    \"\"\"Computes the xy memory time origin for a particle ensemble\n",
    "    and saves it out as a netCDF.\n",
    "\n",
    "    ensemble : xarray object with the particle trajectories for the\n",
    "               given ensemble.\n",
    "    region : str of the region ['drake', 'kerguelan', ...]\n",
    "    \"\"\"\n",
    "    result = xr.apply_ufunc(\n",
    "        deep_tracers_origin,\n",
    "        ensemble.lonParticle,\n",
    "        ensemble.latParticle,\n",
    "        ensemble.zLevelParticle,\n",
    "        ensemble.particleDIC,\n",
    "        ensemble.particleTemperature,\n",
    "        ensemble.particleSalinity,\n",
    "        ensemble.particleALK,\n",
    "        ensemble.particlePO4,\n",
    "        ensemble.particleSiO3,\n",
    "        input_core_dims=[\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "        ],\n",
    "        output_core_dims=[[\"coordinate\"]],\n",
    "        dask_gufunc_kwargs={\"output_sizes\": {\"coordinate\": 10}},\n",
    "        output_dtypes=[float],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "    )\n",
    "    del result[\"coordinate\"]\n",
    "\n",
    "    origin = xr.Dataset()\n",
    "    origin = xr.Dataset()\n",
    "    origin[\"x\"] = np.rad2deg(result.isel(coordinate=0))\n",
    "    origin[\"y\"] = np.rad2deg(result.isel(coordinate=1))\n",
    "    origin[\"z\"] = result.isel(coordinate=2)\n",
    "    origin[\"DIC\"] = result.isel(coordinate=3)\n",
    "    origin[\"T\"] = result.isel(coordinate=4)\n",
    "    origin[\"S\"] = result.isel(coordinate=5)\n",
    "    origin[\"ALK\"] = result.isel(coordinate=6)\n",
    "    origin[\"PO4\"] = result.isel(coordinate=7)\n",
    "    origin[\"SiO3\"] = result.isel(coordinate=8)\n",
    "    origin[\"memory_time\"] = result.isel(coordinate=9).astype(int)\n",
    "    origin.attrs[\n",
    "        \"description\"\n",
    "    ] = f\"tracers origin and memory time based on DIC for the {region} ensemble.\"\n",
    "    origin.attrs[\n",
    "        \"ensemble\"\n",
    "    ] = f\"particles that last upwelled into 1000 m in the {region} region. Filtered to occur S of 45S and out of the 75% sea ice zone. ultimately make it to 200m.\"\n",
    "    print(\"Computing and saving to netCDF...\")\n",
    "    origin.to_netcdf(f\"../data/postproc/{region}.1000m.tracer.origin.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Statistical Source of DIC for Each Region\n",
    "\n",
    "Now we subset our particles into their ensembles, based on their crossing location occurring in a given geographic box. Then we can use the defined functions above to create a dataset that records the (x, y) origin of the DIC that upwells across 1000 m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xCross, yCross = crossings[\"lon_crossing\"], crossings[\"lat_crossing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving to netCDF...\n",
      "CPU times: user 713 ms, sys: 46.4 ms, total: 759 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "region = \"drake\"\n",
    "\n",
    "x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "\n",
    "# Want on a 0-360 longitude scale.\n",
    "x0 += 360\n",
    "x1 += 360\n",
    "\n",
    "drake_conditions = (xCross > x0) & (xCross < x1) & (yCross > y0) & (yCross < y1)\n",
    "drake_ensemble = ds.where(drake_conditions, drop=True)\n",
    "drake_ensemble = drake_ensemble.chunk({\"time\": -1, \"nParticles\": 100}).persist()\n",
    "\n",
    "%time compute_locations_and_save_dataset(drake_ensemble, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving to netCDF...\n",
      "CPU times: user 272 ms, sys: 19.1 ms, total: 291 ms\n",
      "Wall time: 48.7 s\n"
     ]
    }
   ],
   "source": [
    "region = \"crozet\"\n",
    "\n",
    "x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "\n",
    "crozet_conditions = (xCross > x0) & (xCross < x1) & (yCross > y0) & (yCross < y1)\n",
    "crozet_ensemble = ds.where(crozet_conditions, drop=True)\n",
    "crozet_ensemble = crozet_ensemble.chunk({\"time\": -1, \"nParticles\": 100}).persist()\n",
    "%time compute_locations_and_save_dataset(crozet_ensemble, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving to netCDF...\n",
      "CPU times: user 355 ms, sys: 8.94 ms, total: 364 ms\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "region = \"kerguelan\"\n",
    "\n",
    "x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "\n",
    "kerguelan_conditions = (xCross > x0) & (xCross < x1) & (yCross > y0) & (yCross < y1)\n",
    "kerguelan_ensemble = ds.where(kerguelan_conditions, drop=True)\n",
    "kerguelan_ensemble = kerguelan_ensemble.chunk({\"time\": -1, \"nParticles\": 100}).persist()\n",
    "%time compute_locations_and_save_dataset(kerguelan_ensemble, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving to netCDF...\n",
      "CPU times: user 439 ms, sys: 17.5 ms, total: 457 ms\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "region = \"campbell\"\n",
    "\n",
    "x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "\n",
    "campbell_conditions = (xCross > x0) & (xCross < x1) & (yCross > y0) & (yCross < y1)\n",
    "campbell_ensemble = ds.where(campbell_conditions, drop=True)\n",
    "campbell_ensemble = campbell_ensemble.chunk({\"time\": -1, \"nParticles\": 100}).persist()\n",
    "%time compute_locations_and_save_dataset(campbell_ensemble, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing and saving to netCDF...\n",
      "CPU times: user 323 ms, sys: 18.3 ms, total: 341 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "region = \"non_topographic\"\n",
    "# Simply everywhere that is not the other conditions.\n",
    "non_topo_conditions = ~(\n",
    "    drake_conditions + crozet_conditions + kerguelan_conditions + campbell_conditions\n",
    ")\n",
    "non_topo_ensemble = ds.where(non_topo_conditions, drop=True)\n",
    "non_topo_ensemble = non_topo_ensemble.chunk({\"time\": -1, \"nParticles\": 500}).persist()\n",
    "%time compute_locations_and_save_dataset(non_topo_ensemble, region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-brady-carbonpathways]",
   "language": "python",
   "name": "conda-env-miniconda3-brady-carbonpathways-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
