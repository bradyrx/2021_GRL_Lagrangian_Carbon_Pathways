{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.05 Mixed Layer Decorrelation and Residence Time\n",
    "\n",
    "---\n",
    "\n",
    "Author: Riley X. Brady\n",
    "\n",
    "Date: 11/19/2020\n",
    "\n",
    "---\n",
    "\n",
    "_Note_: This code could be a lot cleaner, but it gets the job done.\n",
    "\n",
    "This calculates the decorrelation timescale of DIC once it enters the mixed layer (200 m). We first select the ensemble of particles based on where they first cross 200 m after their last 1000 m crossing. Then for each particle, we evaluate the remainder of the time series following that mixed layer crossing into the given topographic (or non-topographic) region. We assess every single 200 m crossing by the particle, including the first crossing into 200 m, and calculate the decorrelation time scale of DIC during that time.\n",
    "\n",
    "We account for that decorrelation time scale if:\n",
    "\n",
    "* The 200 m crossing happens outside of the annual sea ice edge\n",
    "* The 200 m crossing happens south of 45S\n",
    "* The 200 m crossing happens in waters deeper than 500 m (to avoid shelf-trapped particles)\n",
    "* The autocorrelation is significant with p < 0.05\n",
    "\n",
    "We also discard the given decorrelation time computed if the particle is still above 200 m when the simulation ends.\n",
    "\n",
    "For residence time, we use the same principles, but still clock the residence time even if the decorrelation is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import figutils\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.19.4\n",
      "xarray: 0.16.1\n",
      "scipy: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"xarray: {xr.__version__}\")\n",
    "print(f\"scipy: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my TCP client from the `launch_cluster` notebook. I use it\n",
    "# for distributed computing with `dask` on NCAR's machine, Casper.\n",
    "client = Client(\"tcp://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in information from the Eulerian mesh that will be used when calculating decorrelation and residence time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice = (\n",
    "    xr.open_dataset(\"../data/eulerian_sea_ice_climatology.nc\").mean(\"month\").icePresent\n",
    ")\n",
    "mesh = xr.open_dataset(\"../data/mesh.nc\")\n",
    "depth = xr.open_dataset(\"../data/bottomDepth.nc\")\n",
    "\n",
    "mpas_lat = np.rad2deg(mesh.latCell)\n",
    "mpas_lon = np.rad2deg(mesh.lonCell)\n",
    "bottomDepth = depth.bottomDepth\n",
    "\n",
    "# To save some cost, only look S of 45S\n",
    "# These will be global variables used as a reference in the\n",
    "# decorrelation and residence time calculation.\n",
    "SEA_ICE = ice.where(mpas_lat < -45, drop=True)\n",
    "BOTTOM_DEPTH = bottomDepth.where(mpas_lat < -45, drop=True)\n",
    "MPAS_LON = mpas_lon.where(mpas_lat < -45, drop=True)\n",
    "MPAS_LAT = mpas_lat.where(mpas_lat < -45, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load in all of the deep upwelled particles as a base from which we will subset our ensembles.\n",
    "\n",
    "**Note**: I loaded in the netCDF file, and chunked it, and then saved it back out as a `zarr` file. This makes `dask` run a lot more efficiently. E.g.,\n",
    "\n",
    "```python\n",
    "ds = xr.open_dataset('../data/southern_ocean_deep_upwelling_particles.nc')\n",
    "ds = ds.chunk({'time': -1, 'nParticles': 'auto'})\n",
    "ds.to_zarr('../data/southern_ocean_deep_upwelling_particles.zarr', consolidated=True)\n",
    "```\n",
    "\n",
    "You could probably chunk the particles into slightly smaller chunks for even faster performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the `zarr` file, which is pre-chunked and already has been\n",
    "# filtered from the original 1,000,000 particles to the 19,002 that\n",
    "# upwell last across 1000 m S of 45S and outside of the annual sea ice\n",
    "# edge.\n",
    "filepath = \"../data/southern_ocean_deep_upwelling_particles.zarr/\"\n",
    "ds = xr.open_zarr(filepath, consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk({\"time\": -1, \"nParticles\": 5000})\n",
    "ds = ds.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define all of the functions that will be used to calculate the decorrelation and residence time. These could definitely be cleaned up, but I kind of just want to get this paper submitted! I have found that `apply_ufunc` is sluggish when calling functions that exist in an external `.py` script, so I just define them all here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pearsonr_by_hand(x, lag):\n",
    "    \"\"\"Calculate the pearson r correlation for autocorrelation.\n",
    "\n",
    "    x : Time series to calculate autocorrelation for (particleDIC).\n",
    "    lag : int of the lag for which to compute the autocorrelation.\n",
    "    \"\"\"\n",
    "    y1 = x[: (len(x) - lag)]\n",
    "    y2 = x[lag:]\n",
    "    if len(y1) >= 2:\n",
    "        r, p = pearsonr(y1, y2)\n",
    "        return r, p\n",
    "    else:\n",
    "        # can't compute autocorrelation for 2 points or less.\n",
    "        return np.nan, np.nan\n",
    "\n",
    "\n",
    "def _decorrelation_time(tracer):\n",
    "    \"\"\"Computes decorrelation time (in days) based on the e-folding time.\n",
    "\n",
    "    If p > 0.05, don't return.\n",
    "\n",
    "    tracer : particleDIC or any other tracer for which to compute decorrelation.\n",
    "    \"\"\"\n",
    "    # Find decorrelation time.\n",
    "    auto = np.array([_pearsonr_by_hand(tracer, lag) for lag in np.arange(len(tracer))])\n",
    "    # extract corrcoef and p value.\n",
    "    r = auto[:, 0]\n",
    "    p = auto[:, 1]\n",
    "    e_folding = (r <= 1 / np.e).argmax()\n",
    "    if p[e_folding] < 0.05:\n",
    "        decorr_time = int(e_folding * 2)\n",
    "        return decorr_time\n",
    "    else:\n",
    "        # don't return if non-significant correlation.\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _find_mpas_cell(xParticle, yParticle):\n",
    "    \"\"\"Returns the idx to plug into the MPAS mask array\n",
    "    from above to check whether to keep or not.\n",
    "\n",
    "    We use global MPAS_LON and MPAS_LAT variables here to avoid\n",
    "    having too long of a function signature and passing a bunch of\n",
    "    stuff in and out of apply_ufunc.\n",
    "\n",
    "    xParticle: lonParticle (degrees)\n",
    "    yParticle: latParticle (degrees)\n",
    "    \"\"\"\n",
    "    dx = MPAS_LON - xParticle\n",
    "    dy = MPAS_LAT - yParticle\n",
    "    diff = abs(dx) + abs(dy)\n",
    "    idx = np.nanargmin(diff)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _compute_idx_of_first_200m_crossing(z):\n",
    "    \"\"\"Find first time particle upwells across 200 m.\n",
    "\n",
    "    z : zLevelParticle\n",
    "    \"\"\"\n",
    "    currentDepth = z\n",
    "    previousDepth = np.roll(z, 1)\n",
    "    previousDepth[0] = 999\n",
    "    cond = (currentDepth >= -200) & (previousDepth < -200)\n",
    "    idx = cond.argmax()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _compute_idx_of_last_1000m_crossing(z):\n",
    "    \"\"\"Find index of final time particle upwells across 1000 m.\n",
    "\n",
    "    z : zLevelParticle\n",
    "    \"\"\"\n",
    "    currentDepth = z\n",
    "    previousDepth = np.roll(z, 1)\n",
    "    previousDepth[0] = 999  # So we're not dealing with a nan here.\n",
    "    cond = (currentDepth >= -1000) & (previousDepth < -1000)\n",
    "    idx = (\n",
    "        len(cond) - np.flip(cond).argmax() - 1\n",
    "    )  # Finds last location that condition is true.\n",
    "    return idx\n",
    "\n",
    "\n",
    "def mixed_layer_decorrelation_time(x, y, z, DIC):\n",
    "    \"\"\"Computes the decorrelation time of DIC during the given mixed layer stay.\n",
    "\n",
    "    * Makes sure that the 200 m crossing happens S of 45S\n",
    "    * Makes sure that e-folding autocorrelation coefficient has p < 0.05.\n",
    "    * Makes sure that the given 200 m crossing happens outside of the annual sea ice zone.\n",
    "    * Makes sure that the bottom depth at the given crossing is > 500 m, to avoid coastally\n",
    "      trapped particles that are just oscillating around the mixed layer.\n",
    "    * Throws away the decorrelation if the simulation ends and it's still above 200 m.\n",
    "\n",
    "    x : lonParticle (degrees)\n",
    "    y : latParticle (degrees)\n",
    "    z : zLevelParticle * -1 (m)\n",
    "    DIC : particleDIC\n",
    "    \"\"\"\n",
    "    # Conservative estimate on the max number of 200 m crossings a given particle\n",
    "    # could have. Will fill in one value for each crossing, if applicable.\n",
    "    MIXED_LAYER_DECORRELATION_TIME = np.zeros(200)\n",
    "    MIXED_LAYER_DECORRELATION_TIME[:] = np.nan\n",
    "\n",
    "    # Subset from final 1000 m crossing and beyond.\n",
    "    idx_a = _compute_idx_of_last_1000m_crossing(z * -1)\n",
    "    x = x[idx_a - 1 : :]\n",
    "    y = y[idx_a - 1 : :]\n",
    "    z = z[idx_a - 1 : :]\n",
    "    DIC = DIC[idx_a - 1 : :]\n",
    "\n",
    "    # Find first 200 m crossing after that and subset to\n",
    "    # remainder of trajectory after this.\n",
    "    idx_b = _compute_idx_of_first_200m_crossing(z * -1)\n",
    "    x = x[idx_b - 1 : :]\n",
    "    y = y[idx_b - 1 : :]\n",
    "    z = z[idx_b - 1 : :]\n",
    "    DIC = DIC[idx_b - 1 : :]\n",
    "\n",
    "    # Now we analyze all 200m crossings from this point and beyond.\n",
    "    previous_depth = np.roll(z, 1)\n",
    "\n",
    "    # Don't include first time step since we don't know\n",
    "    # where it was before.\n",
    "    previous_depth = previous_depth[1::]\n",
    "    current_depth = z[1::]\n",
    "\n",
    "    # Find indices where particle upwells into 200m. Looking for all occurrences.\n",
    "    (mixed_layer_idxs,) = np.where((previous_depth > 200) & (current_depth < 200))\n",
    "    # Account for `np.roll(...)`\n",
    "    mixed_layer_idxs += 1\n",
    "\n",
    "    # Only maintain those that upwell S of 45S\n",
    "    mixed_layer_idxs = mixed_layer_idxs[y[mixed_layer_idxs] < -45]\n",
    "\n",
    "    for filler, idx in enumerate(mixed_layer_idxs):\n",
    "        cellidx = _find_mpas_cell(x[idx], y[idx])\n",
    "\n",
    "        # Check that particle crosses into 200 m outside of sea ice zone\n",
    "        # and in waters deeper than 500 m.\n",
    "        if (SEA_ICE[cellidx] < 0.75) and (BOTTOM_DEPTH[cellidx] > 500):\n",
    "            zsubset = z[idx::]\n",
    "            dicsubset = DIC[idx::]\n",
    "            time_steps_below_mixed_layer = np.argwhere(zsubset > 200)\n",
    "\n",
    "            # If this isn't True, it stays above 200m for remainder of trajectory\n",
    "            # and we toss it away, leaving it as a NaN.\n",
    "            if time_steps_below_mixed_layer.any():\n",
    "                idx_of_next_subduction = time_steps_below_mixed_layer.min()\n",
    "\n",
    "                mixed_layer_dic_subset = dicsubset[0:idx_of_next_subduction]\n",
    "                # returns integer number of days for decorr time\n",
    "                # (e-folding decorrelation time * 2 days per time step on average)\n",
    "                decorr = _decorrelation_time(mixed_layer_dic_subset)\n",
    "\n",
    "                # Not possible since at time step 0 it's exactly 1. This just\n",
    "                # means it didn't decorr over how long it was up here.\n",
    "                if decorr != 0:\n",
    "                    MIXED_LAYER_DECORRELATION_TIME[filler] = decorr\n",
    "    return MIXED_LAYER_DECORRELATION_TIME\n",
    "\n",
    "\n",
    "def mixed_layer_residence_time(x, y, z):\n",
    "    \"\"\"Computes the residence time of a particle during the given mixed layer stay.\n",
    "\n",
    "    * Makes sure that the 200 m crossing happens S of 45S\n",
    "    * Makes sure that e-folding autocorrelation coefficient has p < 0.05.\n",
    "    * Makes sure that the given 200 m crossing happens outside of the annual sea ice zone.\n",
    "    * Makes sure that the bottom depth at the given crossing is > 500 m, to avoid coastally\n",
    "      trapped particles that are just oscillating around the mixed layer.\n",
    "    * Throws away the decorrelation if the simulation ends and it's still above 200 m.\n",
    "\n",
    "    x : lonParticle (degrees)\n",
    "    y : latParticle (degrees)\n",
    "    z : zLevelParticle * -1 (m)\n",
    "    \"\"\"\n",
    "    # Conservative estimate on the max number of 200 m crossings a given particle\n",
    "    # could have. Will fill in one value for each crossing, if applicable.\n",
    "    MIXED_LAYER_RESIDENCE_TIME = np.zeros(200)\n",
    "    MIXED_LAYER_RESIDENCE_TIME[:] = np.nan\n",
    "\n",
    "    # Subset from final 1000 m crossing and beyond.\n",
    "    idx_a = _compute_idx_of_last_1000m_crossing(z * -1)\n",
    "    x = x[idx_a - 1 : :]\n",
    "    y = y[idx_a - 1 : :]\n",
    "    z = z[idx_a - 1 : :]\n",
    "\n",
    "    # Find first 200 m crossing after that and subset to\n",
    "    # remainder of trajectory after this.\n",
    "    idx_b = _compute_idx_of_first_200m_crossing(z * -1)\n",
    "    x = x[idx_b - 1 : :]\n",
    "    y = y[idx_b - 1 : :]\n",
    "    z = z[idx_b - 1 : :]\n",
    "\n",
    "    # Now we analyze all 200m crossings from this point and beyond.\n",
    "    previous_depth = np.roll(z, 1)\n",
    "\n",
    "    # Don't include first time step since we don't know\n",
    "    # where it was before.\n",
    "    previous_depth = previous_depth[1::]\n",
    "    current_depth = z[1::]\n",
    "\n",
    "    # Find indices where particle upwells into 200m. Looking for all occurrences.\n",
    "    (mixed_layer_idxs,) = np.where((previous_depth > 200) & (current_depth < 200))\n",
    "    # Account for `np.roll(...)`\n",
    "    mixed_layer_idxs += 1\n",
    "\n",
    "    # Only maintain those that upwell S of 45S\n",
    "    mixed_layer_idxs = mixed_layer_idxs[y[mixed_layer_idxs] < -45]\n",
    "\n",
    "    for filler, idx in enumerate(mixed_layer_idxs):\n",
    "        cellidx = _find_mpas_cell(x[idx], y[idx])\n",
    "\n",
    "        # Check that particle crosses into 200 m outside of sea ice zone\n",
    "        # and in waters deeper than 500 m.\n",
    "        if (SEA_ICE[cellidx] < 0.75) and (BOTTOM_DEPTH[cellidx] > 500):\n",
    "            zsubset = z[idx::]\n",
    "            time_steps_below_mixed_layer = np.argwhere(zsubset > 200)\n",
    "\n",
    "            # If this isn't True, it stays above 200m for remainder of trajectory\n",
    "            # and we toss it away, leaving it as a NaN.\n",
    "            if time_steps_below_mixed_layer.any():\n",
    "                idx_of_next_subduction = time_steps_below_mixed_layer.min()\n",
    "                mixed_layer_z_subset = zsubset[0:idx_of_next_subduction]\n",
    "                MIXED_LAYER_RESIDENCE_TIME[filler] = int(len(mixed_layer_z_subset) * 2)\n",
    "    return MIXED_LAYER_RESIDENCE_TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorrelation Time Calculations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topographic Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drake...\n",
      "CPU times: user 238 ms, sys: 50.9 ms, total: 289 ms\n",
      "Wall time: 27.2 s\n",
      "crozet...\n",
      "CPU times: user 104 ms, sys: 41 ms, total: 145 ms\n",
      "Wall time: 26.3 s\n",
      "kerguelan...\n",
      "CPU times: user 138 ms, sys: 31.3 ms, total: 169 ms\n",
      "Wall time: 24.9 s\n",
      "campbell...\n",
      "CPU times: user 83.5 ms, sys: 29 ms, total: 112 ms\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "crossings = xr.open_dataset(\"../data/postproc/200m.crossing.locations.nc\")\n",
    "xc, yc = crossings[\"lon_crossing\"], crossings[\"lat_crossing\"]\n",
    "\n",
    "for region in [\"drake\", \"crozet\", \"kerguelan\", \"campbell\"]:\n",
    "    print(f\"{region}...\")\n",
    "    x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "    if region == \"drake\":\n",
    "        x0 += 360\n",
    "        x1 += 360\n",
    "\n",
    "    conditions = (xc > x0) & (xc < x1) & (yc > y0) & (yc < y1)\n",
    "    particle_ids = conditions.where(conditions, drop=True).nParticles.astype(int)\n",
    "\n",
    "    # Select ensemble based on 200m crossing location.\n",
    "    ensemble = ds.sel(nParticles=particle_ids)\n",
    "    ensemble = ensemble.chunk({\"time\": -1, \"nParticles\": 250}).persist()\n",
    "\n",
    "    # Add some helpful variables\n",
    "    ensemble[\"depth\"] = ensemble.zLevelParticle * -1\n",
    "    ensemble[\"latDegrees\"] = np.rad2deg(ensemble.latParticle)\n",
    "    ensemble[\"lonDegrees\"] = np.rad2deg(ensemble.lonParticle)\n",
    "\n",
    "    # Calculate decorrelation time for every mixed layer instance\n",
    "    # following that first 200 m crossing.\n",
    "    decorr_result = xr.apply_ufunc(\n",
    "        mixed_layer_decorrelation_time,\n",
    "        ensemble.lonDegrees,\n",
    "        ensemble.latDegrees,\n",
    "        ensemble.depth,\n",
    "        ensemble.particleDIC,\n",
    "        input_core_dims=[[\"time\"], [\"time\"], [\"time\"], [\"time\"]],\n",
    "        output_core_dims=[[\"crossings\"]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs={\"output_sizes\": {\"crossings\": 200}},\n",
    "        output_dtypes=[float],\n",
    "    )\n",
    "\n",
    "    %time decorr_result = decorr_result.compute()\n",
    "\n",
    "    # Create single dimension of all crossings for the given ensemble.\n",
    "    decorr_result = decorr_result.stack(\n",
    "        all_crossings=[\"nParticles\", \"crossings\"]\n",
    "    ).dropna(\"all_crossings\")\n",
    "    decorr_result = decorr_result.rename(\"decorr\").to_dataset()\n",
    "    decorr_result.attrs[\n",
    "        \"description\"\n",
    "    ] = \"surface DIC decorrelation time for every 200 m crossing after the first mixed layer crossing for the given particle ensemble.\"\n",
    "    decorr_result.attrs[\n",
    "        \"dropped_cases\"\n",
    "    ] = \"inside sea ice zone; N of 45S; p > 0.05 autocorrelation; in waters shallower than 500m; simulation ends with particle above 200m\"\n",
    "    decorr_result.reset_index(\"all_crossings\").to_netcdf(\n",
    "        f\"../data/postproc/{region}.DIC.decorr.nc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Topographic Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 243 ms, sys: 82.1 ms, total: 326 ms\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "base_conditions = crossings.nParticles < 0  # just creates an all False bool.\n",
    "for region in [\"drake\", \"crozet\", \"kerguelan\", \"campbell\"]:\n",
    "    x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "    if region == \"drake\":\n",
    "        x0 += 360\n",
    "        x1 += 360\n",
    "\n",
    "    conditions = (xc > x0) & (xc < x1) & (yc > y0) & (yc < y1)\n",
    "    base_conditions = base_conditions + conditions\n",
    "\n",
    "# Used the above as a quick way to get at the particle IDs for the non-topographic\n",
    "# particles.\n",
    "particle_ids = crossings.where(~base_conditions, drop=True).nParticles.astype(int)\n",
    "ensemble = ds.sel(nParticles=particle_ids)\n",
    "ensemble = ensemble.chunk({\"time\": -1, \"nParticles\": 250}).persist()\n",
    "\n",
    "# Add some helpful variables\n",
    "ensemble[\"depth\"] = ensemble.zLevelParticle * -1\n",
    "ensemble[\"latDegrees\"] = np.rad2deg(ensemble.latParticle)\n",
    "ensemble[\"lonDegrees\"] = np.rad2deg(ensemble.lonParticle)\n",
    "\n",
    "# Calculate decorrelation time for every mixed layer instance\n",
    "# following that first 200 m crossing.\n",
    "decorr_result = xr.apply_ufunc(\n",
    "    mixed_layer_decorrelation_time,\n",
    "    ensemble.lonDegrees,\n",
    "    ensemble.latDegrees,\n",
    "    ensemble.depth,\n",
    "    ensemble.particleDIC,\n",
    "    input_core_dims=[[\"time\"], [\"time\"], [\"time\"], [\"time\"]],\n",
    "    output_core_dims=[[\"crossings\"]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs={\"output_sizes\": {\"crossings\": 200}},\n",
    "    output_dtypes=[float],\n",
    ")\n",
    "\n",
    "%time decorr_result = decorr_result.compute()\n",
    "\n",
    "# Create single dimension of all crossings for the given ensemble.\n",
    "decorr_result = decorr_result.stack(all_crossings=[\"nParticles\", \"crossings\"]).dropna(\n",
    "    \"all_crossings\"\n",
    ")\n",
    "decorr_result = decorr_result.rename(\"decorr\").to_dataset()\n",
    "decorr_result.attrs[\n",
    "    \"description\"\n",
    "] = \"surface DIC decorrelation time for every 200 m crossing after the first mixed layer crossing for the given particle ensemble.\"\n",
    "decorr_result.attrs[\n",
    "    \"dropped_cases\"\n",
    "] = \"inside sea ice zone; N of 45S; p > 0.05 autocorrelation; in waters shallower than 500m; simulation ends with particle above 200m\"\n",
    "decorr_result.reset_index(\"all_crossings\").to_netcdf(\n",
    "    \"../data/postproc/non_topographic.DIC.decorr.nc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residence Time Calculations \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topographic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drake...\n",
      "CPU times: user 183 ms, sys: 57.5 ms, total: 241 ms\n",
      "Wall time: 19.4 s\n",
      "crozet...\n",
      "CPU times: user 82.9 ms, sys: 26.1 ms, total: 109 ms\n",
      "Wall time: 13.6 s\n",
      "kerguelan...\n",
      "CPU times: user 119 ms, sys: 30.8 ms, total: 150 ms\n",
      "Wall time: 14.9 s\n",
      "campbell...\n",
      "CPU times: user 71.6 ms, sys: 23.9 ms, total: 95.5 ms\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "crossings = xr.open_dataset(\"../data/postproc/200m.crossing.locations.nc\")\n",
    "xc, yc = crossings[\"lon_crossing\"], crossings[\"lat_crossing\"]\n",
    "\n",
    "for region in [\"drake\", \"crozet\", \"kerguelan\", \"campbell\"]:\n",
    "    print(f\"{region}...\")\n",
    "    x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "    if region == \"drake\":\n",
    "        x0 += 360\n",
    "        x1 += 360\n",
    "\n",
    "    conditions = (xc > x0) & (xc < x1) & (yc > y0) & (yc < y1)\n",
    "    particle_ids = conditions.where(conditions, drop=True).nParticles.astype(int)\n",
    "\n",
    "    # Select ensemble based on 200m crossing location.\n",
    "    ensemble = ds.sel(nParticles=particle_ids)\n",
    "    ensemble = ensemble.chunk({\"time\": -1, \"nParticles\": 250}).persist()\n",
    "\n",
    "    # Add some helpful variables\n",
    "    ensemble[\"depth\"] = ensemble.zLevelParticle * -1\n",
    "    ensemble[\"latDegrees\"] = np.rad2deg(ensemble.latParticle)\n",
    "    ensemble[\"lonDegrees\"] = np.rad2deg(ensemble.lonParticle)\n",
    "\n",
    "    # Calculate residence time for every mixed layer instance\n",
    "    # following that first 200 m crossing.\n",
    "    tau_result = xr.apply_ufunc(\n",
    "        mixed_layer_residence_time,\n",
    "        ensemble.lonDegrees,\n",
    "        ensemble.latDegrees,\n",
    "        ensemble.depth,\n",
    "        input_core_dims=[\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "            [\"time\"],\n",
    "        ],\n",
    "        output_core_dims=[[\"crossings\"]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        dask_gufunc_kwargs={\"output_sizes\": {\"crossings\": 200}},\n",
    "        output_dtypes=[float],\n",
    "    )\n",
    "\n",
    "    %time tau_result = tau_result.compute()\n",
    "\n",
    "    # Create single dimension of all crossings for the given ensemble.\n",
    "    tau_result = tau_result.stack(all_crossings=[\"nParticles\", \"crossings\"]).dropna(\n",
    "        \"all_crossings\"\n",
    "    )\n",
    "    tau_result = tau_result.rename(\"tau\").to_dataset()\n",
    "    tau_result.attrs[\n",
    "        \"description\"\n",
    "    ] = \"mixed layer residence time for every 200 m crossing after the first mixed layer crossing for the given particle ensemble.\"\n",
    "    tau_result.attrs[\n",
    "        \"dropped_cases\"\n",
    "    ] = \"inside sea ice zone; N of 45S; in waters shallower than 500m; simulation ends with particle above 200m\"\n",
    "    tau_result.reset_index(\"all_crossings\").to_netcdf(\n",
    "        f\"../data/postproc/{region}.tau.surface.nc\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-topographic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 68.1 ms, total: 284 ms\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "base_conditions = crossings.nParticles < 0  # just creates an all False bool.\n",
    "for region in [\"drake\", \"crozet\", \"kerguelan\", \"campbell\"]:\n",
    "    x0, x1, y0, y1 = figutils.BOUNDS[region]\n",
    "    if region == \"drake\":\n",
    "        x0 += 360\n",
    "        x1 += 360\n",
    "\n",
    "    conditions = (xc > x0) & (xc < x1) & (yc > y0) & (yc < y1)\n",
    "    base_conditions = base_conditions + conditions\n",
    "\n",
    "# Used the above as a quick way to get at the particle IDs for the non-topographic\n",
    "# particles.\n",
    "particle_ids = crossings.where(~base_conditions, drop=True).nParticles.astype(int)\n",
    "# Select ensemble based on 200m crossing location.\n",
    "ensemble = ds.sel(nParticles=particle_ids)\n",
    "ensemble = ensemble.chunk({\"time\": -1, \"nParticles\": 250}).persist()\n",
    "\n",
    "# Add some helpful variables\n",
    "ensemble[\"depth\"] = ensemble.zLevelParticle * -1\n",
    "ensemble[\"latDegrees\"] = np.rad2deg(ensemble.latParticle)\n",
    "ensemble[\"lonDegrees\"] = np.rad2deg(ensemble.lonParticle)\n",
    "\n",
    "# Calculate residence time for every mixed layer instance\n",
    "# following that first 200 m crossing.\n",
    "tau_result = xr.apply_ufunc(\n",
    "    mixed_layer_residence_time,\n",
    "    ensemble.lonDegrees,\n",
    "    ensemble.latDegrees,\n",
    "    ensemble.depth,\n",
    "    input_core_dims=[\n",
    "        [\"time\"],\n",
    "        [\"time\"],\n",
    "        [\"time\"],\n",
    "    ],\n",
    "    output_core_dims=[[\"crossings\"]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    dask_gufunc_kwargs={\"output_sizes\": {\"crossings\": 200}},\n",
    "    output_dtypes=[float],\n",
    ")\n",
    "\n",
    "%time tau_result = tau_result.compute()\n",
    "\n",
    "# Create single dimension of all crossings for the given ensemble.\n",
    "tau_result = tau_result.stack(all_crossings=[\"nParticles\", \"crossings\"]).dropna(\n",
    "    \"all_crossings\"\n",
    ")\n",
    "tau_result = tau_result.rename(\"tau\").to_dataset()\n",
    "tau_result.attrs[\n",
    "    \"description\"\n",
    "] = \"mixed layer residence time for every 200 m crossing after the first mixed layer crossing for the given particle ensemble.\"\n",
    "tau_result.attrs[\n",
    "    \"dropped_cases\"\n",
    "] = \"inside sea ice zone; N of 45S; in waters shallower than 500m; simulation ends with particle above 200m\"\n",
    "tau_result.reset_index(\"all_crossings\").to_netcdf(\n",
    "    f\"../data/postproc/non_topographic.tau.surface.nc\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-brady-carbonpathways]",
   "language": "python",
   "name": "conda-env-miniconda3-brady-carbonpathways-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
